{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a73255e",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e2e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9d6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4ALL_MODEL_PATH = \"E:\\\\gpt4all-main\\\\chat\\\\gpt4all-converted.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d06f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Answer: Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f673021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(model_path=GPT4ALL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e59d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3128204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "llama_embeddings = LlamaCppEmbeddings(model_path=GPT4ALL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bd72202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"C:\\\\Users\\\\PC\\\\Downloads\\\\NLP_A5\\\\data\\\\textual_FAST.txt\")\n",
    "\n",
    "\n",
    "\n",
    "urls = [\n",
    "    'https://www.nu.edu.pk/',\n",
    "    'https://www.nu.edu.pk/Degree-Programs',\n",
    "    'https://www.nu.edu.pk/Admissions/Schedule', 'https://www.nu.edu.pk/Admissions/HowToApply', 'https://www.nu.edu.pk/Admissions/EligibilityCriteria', 'https://www.nu.edu.pk/Admissions/Scholarship', 'https://www.nu.edu.pk/Admissions/TestPattern', 'https://www.nu.edu.pk/Admissions/FeeStructure', 'http://isb.nu.edu.pk/', 'http://isb.nu.edu.pk/Faculty/allfaculty#cs', 'http://isb.nu.edu.pk/Faculty/allfaculty#ms', 'http://isb.nu.edu.pk/Faculty/allfaculty#ee', 'http://isb.nu.edu.pk/Faculty/allfaculty#sh', 'http://isb.nu.edu.pk/Student/Grading', 'https://nu.edu.pk/Student/Calender', 'https://nu.edu.pk/Student/Conduct' , 'https://nu.edu.pk/Student/HECEquivalence',\n",
    "'https://nu.edu.pk/Student/FinancialRules','https://nu.edu.pk/University/History', 'https://nu.edu.pk/University/Foundation' , 'https://nu.edu.pk/University/Chancellor', 'https://nu.edu.pk/vision-and-mission' , 'https://nu.edu.pk/University/Trustees' , 'https://nu.edu.pk/University/Governers' , 'https://nu.edu.pk/University/Officers' , 'https://nu.edu.pk/University/Headquarters', 'https://nu.edu.pk/University/PhDFaculty' , 'https://nu.edu.pk/University/HECSupervisors' , 'https://nu.edu.pk/home/ContactUs'\n",
    "]\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "loader = WebBaseLoader(urls)\n",
    "scrape_data = loader.aload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d16e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ad17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db\n"
     ]
    }
   ],
   "source": [
    "# NOTE: must specify a persist_directory oncreation to persist the collection\n",
    "\n",
    "index = VectorstoreIndexCreator(embedding=llama_embeddings,\n",
    "                                vectorstore_kwargs={\"persist_directory\": \"db\"}\n",
    "                               ).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61649b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is FAST NUCES?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1a113a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Again, we should persist the db and figure out how to reuse it\n",
    "docsearch = Chroma.from_documents(texts, llama_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2db85307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just getting a single result document from the knowledge lookup is fine...\n",
    "MIN_DOCS = 1\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",\n",
    "                                 retriever=docsearch.as_retriever(search_kwargs={\"k\": MIN_DOCS}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8eafa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is FAST NUCES?\n",
      " Founded as a Federally Chartered University in July 2000, the National University of Computer and Emerging Sciences is a premiere University of Pakistan, renowned for quality and impact of its students in the development of local software and other industries. The university has five modern campuses at Karachi, Lahore, Islamabad, Peshawar and Chiniot-Faisalabad. These campuses provide world class educational environment and recreational facilities to about over 11,000 students, around one quarter are female and over 500 skilled faculty members.\n"
     ]
    }
   ],
   "source": [
    "print(query)\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1d99886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class Chatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        self.index = index\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        response = index.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc193360",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Chatbot(index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    response = bot.generate_response(user_input)\n",
    "    print(f\"RAZ: {response['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3966f0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
